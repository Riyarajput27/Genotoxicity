# -*- coding: utf-8 -*-
"""FP+DES.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FIsrIdB87IW80iw_onUHBJd26LOoMpyE
"""

#CELL01
#Step 1: Install RDKit and Openpyxl
!pip install rdkit-pypi openpyxl
!pip install rdkit-pypi mordred openpyxl
!pip install pubchempy

"""**FINGERPRINT PROCESSING**"""

#CELL02
import pandas as pd
import numpy as np
from rdkit import Chem
from rdkit.Chem import AllChem, MACCSkeys
from rdkit.Chem.EState import Fingerprinter

# Load the Excel file
xlsx_file = '/content/DATASET_insilco_prediction.xlsx'
df = pd.read_excel(xlsx_file)

# Ensure the file contains a column named 'SMILES'
if 'SMILES' not in df.columns:
    raise ValueError("The Excel file must contain a column named 'SMILES'.")

# Function to calculate fingerprints
def calculate_fingerprints(smiles):
    mol = Chem.MolFromSmiles(smiles)
    if not mol:
        return None
    fingerprints = {}
    # CDK fingerprint (Morgan Fingerprint)
    fingerprints['CDK_FP'] = list(AllChem.GetMorganFingerprintAsBitVect(mol, 2, nBits=1024))
    # CDK Extended fingerprint (Morgan Fingerprint with features)
    fingerprints['CDK_ExtFP'] = list(AllChem.GetMorganFingerprintAsBitVect(mol, 2, useFeatures=True, nBits=1024))
    # Estate fingerprint (Estate)
    estate_fp = Fingerprinter.FingerprintMol(mol)[0]
    fingerprints['Estate_FP'] = [int(x) for x in estate_fp]
    # MACCS fingerprint (MACCS)
    fingerprints['MACCS_FP'] = list(MACCSkeys.GenMACCSKeys(mol))
    # Substructure fingerprint (SubFP)
    sub_fp = Chem.PatternFingerprint(mol)
    fingerprints['SubFP'] = list(sub_fp)
    # PubChem-like fingerprint (using RDKit ECFP)
    pubchem_like_fp = AllChem.GetHashedAtomPairFingerprintAsBitVect(mol, nBits=881)
    fingerprints['PubChem_FP'] = list(pubchem_like_fp)

    return fingerprints

# Apply the function to each molecule in the dataframe
fingerprints_data = df['SMILES'].apply(calculate_fingerprints)

# Convert the fingerprints data into a DataFrame
fingerprints_df = pd.DataFrame(fingerprints_data.tolist())

# Combine the original dataframe with the fingerprints dataframe
result_df = pd.concat([df, fingerprints_df], axis=1)

# Remove rows with missing values in the 'SMILES' column
result_df = result_df.dropna(subset=['SMILES'])

# Ensure all column names are strings
result_df.columns = result_df.columns.astype(str)

# Save the fingerprints data to a new Excel file
result_df.to_excel('fingerprints_output.xlsx', index=False)

# Display the resulting dataframe
print(result_df.head())

#CELL03
import pandas as pd
import numpy as np

# Load the dataset
file_path = '/content/fingerprints_output.xlsx'
df = pd.read_excel(file_path)

# Drop the 'No.' column
df = df.drop(columns=['No.'])

# Select only numeric columns
numeric_df = df.select_dtypes(include=[float, int])

# Remove columns with all zero values
numeric_df = numeric_df.loc[:, (numeric_df != 0).any(axis=0)]

# Remove columns with variance lower than 0.05
numeric_df = numeric_df.loc[:, numeric_df.var() >= 0.05]

# Calculate pairwise correlations
correlation_matrix = numeric_df.corr().abs()

# Select upper triangle of correlation matrix
upper_triangle = correlation_matrix.where(
    np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)
)

# Find index of feature columns with correlation greater than 0.95
to_drop = [
    column for column in upper_triangle.columns if any(upper_triangle[column] > 0.95)
]

# Drop highly correlated columns
numeric_df = numeric_df.drop(columns=to_drop)

# Concatenate non-numeric columns back with cleaned numeric data (if any non-numeric columns are needed)
non_numeric_df = df.select_dtypes(exclude=[float, int])
cleaned_df = pd.concat([non_numeric_df, numeric_df], axis=1)

# Save the cleaned dataset
cleaned_file_path = 'cleaned_fingerprint_output.xlsx'
cleaned_df.to_excel(cleaned_file_path, index=False)

print("Redundant descriptors with correlation values higher than 0.95 have been removed.")

#CELL04
import pandas as pd
from sklearn.ensemble import RandomForestClassifier

# Load the cleaned dataset
cleaned_file_path = '/content/cleaned_fingerprint_output.xlsx'
cleaned_df = pd.read_excel(cleaned_file_path)

# Assuming 'target_column' contains the column name of your target variable
target_column = 'Classification'

# Separate features and target variable
X = cleaned_df.drop(columns=[target_column])
y = cleaned_df[target_column]

# Encode categorical columns using one-hot encoding
X_encoded = pd.get_dummies(X)

# Initialize Random Forest Classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

# Fit the model
rf_classifier.fit(X_encoded, y)

# Get feature importances
feature_importances = rf_classifier.feature_importances_

# Create a DataFrame to store feature importances
importance_df = pd.DataFrame({'Feature': X_encoded.columns, 'Importance': feature_importances})

# Sort the DataFrame by importance
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Filter descriptors with importance value of 0 and count the number of descriptors removed
filtered_importance_df = importance_df[importance_df['Importance'] != 0]
num_descriptors_removed = len(importance_df) - len(filtered_importance_df)

# Find minimum and maximum importance values
min_importance = importance_df['Importance'].min()
max_importance = importance_df['Importance'].max()

print(f"Minimum Importance: {min_importance}")
print(f"Maximum Importance: {max_importance}")

# Total number of descriptors
total_descriptors = len(X_encoded.columns)
print(f"Total number of features: {total_descriptors}")

# Print the number of descriptors removed
print(f"Number of features removed: {num_descriptors_removed}")

# Print or visualize the filtered feature importances
print(filtered_importance_df)

# Save filtered feature importances to a file
filtered_importance_file_path = '/content/filtered_FP_feature_importances.csv'
filtered_importance_df.to_csv(filtered_importance_file_path, index=False)

# Get the column names to drop
columns_to_drop = filtered_importance_df['Feature'].tolist()

# Drop columns from the dataset
X_filtered = X_encoded.drop(columns=columns_to_drop)

# Save the modified dataset to a new Excel file
filtered_file_path = '/content/filtered_FP_dataset.xlsx'
X_filtered.to_excel(filtered_file_path, index=False)

print(f"Filtered dataset saved to: {filtered_file_path}")
print(f"Filtered feature importance CSV file saved to: {filtered_importance_file_path}")



#CELL05
import pandas as pd
from sklearn.feature_selection import RFE
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score, StratifiedKFold

# Load the cleaned dataset
cleaned_file_path = '/content/cleaned_fingerprint_output.xlsx'
cleaned_df = pd.read_excel(cleaned_file_path)

# Assuming 'Classification' is your target variable
target_column = 'Classification'

# Separate features and target variable
X = cleaned_df.drop(columns=[target_column])
y = cleaned_df[target_column]

# Encode categorical columns using one-hot encoding
X_encoded = pd.get_dummies(X)

# Initialize SVM model with linear kernel
svm_model = SVC(kernel='linear', random_state=42)

# Initialize RFE with the SVM model
rfe = RFE(estimator=svm_model)

# Set up cross-validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Perform RFE with cross-validation to select the optimal number of features
n_features_to_select = None  # None means keep all features
rfe.fit(X_encoded, y)

# Use cross-validation to evaluate the model
scores = cross_val_score(rfe, X_encoded, y, cv=cv)

# Print the cross-validation scores
print(f"Cross-validation scores: {scores}")
print(f"Mean cross-validation score: {scores.mean()}")

# Get the support (selected features) and ranking of features
selected_features = X_encoded.columns[rfe.support_]
ranking = rfe.ranking_

# Create a DataFrame to store the feature rankings
rfe_ranking_df = pd.DataFrame({'Feature': X_encoded.columns, 'Ranking': ranking})

# Sort the DataFrame by ranking (1 means selected feature)
rfe_ranking_df = rfe_ranking_df.sort_values(by='Ranking')

# Print or visualize the RFE rankings
print(rfe_ranking_df)

# Save the RFE rankings to a file
rfe_ranking_file_path = '/content/rfe_FP_feature_rankings.csv'
rfe_ranking_df.to_csv(rfe_ranking_file_path, index=False)

# Drop unselected features from the dataset based on RFE
X_rfe_selected = X_encoded[selected_features]

# Save the modified dataset with selected features to a new Excel file
selected_features_file_path = '/content/rfe_FP_selected_features_dataset.xlsx'
X_rfe_selected.to_excel(selected_features_file_path, index=False)

print(f"RFE selected features dataset saved to: {selected_features_file_path}")
print(f"RFE feature ranking CSV file saved to: {rfe_ranking_file_path}")

#CELL06
#Import Required Libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold
from sklearn.metrics import roc_auc_score
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier

#Load the Dataset with Selected Features:
# Load the dataset with selected features
selected_features_file_path = '/content/rfe_FP_selected_features_dataset.xlsx'
selected_df = pd.read_excel(selected_features_file_path)

# Load the original target column from the cleaned dataset
cleaned_file_path = '/content/cleaned_fingerprint_output.xlsx'
cleaned_df = pd.read_excel(cleaned_file_path)
target_column = 'Classification'

# Ensure target variable is the same length as selected features data
y = cleaned_df[target_column][:len(selected_df)]
X = selected_df

#define Models and Parameter Grids:
models = {
    'SVM': SVC(probability=True),
    'Naive Bayes': GaussianNB(),
    'kNN': KNeighborsClassifier(),
    'Decision Tree': DecisionTreeClassifier(),
    'Random Forest': RandomForestClassifier(),
    'ANN': MLPClassifier(max_iter=1000)
}

param_grids = {
    'SVM': {'kernel': ['rbf'], 'C': [0.1, 1, 10], 'gamma': [0.01, 0.1, 1]},
    'Naive Bayes': {},  # No hyperparameters to optimize
    'kNN': {'n_neighbors': [3, 5, 7, 9]},
    'Decision Tree': {'criterion': ['gini', 'entropy'], 'max_depth': [None, 10, 20, 30]},
    'Random Forest': {'n_estimators': [50, 100, 200], 'criterion': ['gini', 'entropy']},
    'ANN': {'hidden_layer_sizes': [(50,), (100,), (50, 50)], 'activation': ['tanh', 'relu'], 'solver': ['adam', 'sgd']}
}

#CELL07
#Model Building and Evaluation:
results = {}
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

for model_name in models:
    print(f"Training {model_name}...")
    model = models[model_name]
    param_grid = param_grids[model_name]

    if param_grid:
        grid_search = GridSearchCV(model, param_grid, cv=cv, scoring='roc_auc')
        grid_search.fit(X, y)
        best_model = grid_search.best_estimator_
        best_params = grid_search.best_params_
    else:
        best_model = model
        best_model.fit(X, y)
        best_params = {}

    # Cross-validated AUC score
    auc_scores = cross_val_score(best_model, X, y, cv=cv, scoring='roc_auc')
    results[model_name] = {'best_params': best_params, 'mean_auc': np.mean(auc_scores)}

# Display results
for model_name, result in results.items():
    print(f"{model_name}: Best Params - {result['best_params']}, Mean AUC - {result['mean_auc']:.4f}")

#Summary and Visualization
import matplotlib.pyplot as plt

model_names = list(results.keys())
mean_aucs = [results[name]['mean_auc'] for name in model_names]

plt.barh(model_names, mean_aucs, color='skyblue')
plt.xlabel('Mean AUC')
plt.title('Model Comparison based on AUC')
plt.show()

#Save Results:
results_df = pd.DataFrame(results).T
results_file_path = '/content/final_FP_model_results.xlsx'
results_df.to_excel(results_file_path)

print(f"Model results saved to: {results_file_path}")

#CELL08

#Define Metric Calculation Functions
from sklearn.metrics import confusion_matrix, roc_auc_score

def calculate_metrics(y_true, y_pred):
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    accuracy = (tp + tn) / (tp + tn + fp + fn)
    specificity = tn / (tn + fp)
    sensitivity = tp / (tp + fn)

    return {
        'TP': tp,
        'TN': tn,
        'FP': fp,
        'FN': fn,
        'Accuracy': accuracy,
        'Specificity': specificity,
        'Sensitivity': sensitivity
    }

def evaluate_model(model, X, y, cv):
    auc_scores = cross_val_score(model, X, y, cv=cv, scoring='roc_auc')
    preds = cross_val_score(model, X, y, cv=cv, scoring='accuracy')

    return {
        'AUC': np.mean(auc_scores),
        'Accuracy': np.mean(preds)
    }

#Calculate Metrics for Each Model Using Cross-Validation
results = {}
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

for model_name in models:
    print(f"Evaluating {model_name}...")
    model = models[model_name]
    param_grid = param_grids[model_name]

    if param_grid:
        grid_search = GridSearchCV(model, param_grid, cv=cv, scoring='roc_auc')
        grid_search.fit(X, y)
        best_model = grid_search.best_estimator_
        best_params = grid_search.best_params_
    else:
        best_model = model
        best_model.fit(X, y)
        best_params = {}

    # Perform cross-validation to calculate AUC and other metrics
    evaluation_scores = evaluate_model(best_model, X, y, cv)

    # Fit model on full data for confusion matrix metrics
    best_model.fit(X, y)
    y_pred = best_model.predict(X)
    metrics = calculate_metrics(y, y_pred)
    metrics['AUC'] = evaluation_scores['AUC']

    results[model_name] = {
        'best_params': best_params,
        'metrics': metrics
    }

# Display results
for model_name, result in results.items():
    metrics = result['metrics']
    print(f"\n{model_name} - Best Params: {result['best_params']}")
    print(f"AUC: {metrics['AUC']:.4f}")
    print(f"Accuracy: {metrics['Accuracy']:.4f}")
    print(f"Specificity: {metrics['Specificity']:.4f}")
    print(f"Sensitivity: {metrics['Sensitivity']:.4f}")
    print(f"TP: {metrics['TP']}, TN: {metrics['TN']}, FP: {metrics['FP']}, FN: {metrics['FN']}")

# Summary and Save Results
results_df = pd.DataFrame.from_dict({model: data['metrics'] for model, data in results.items()}, orient='index')
results_file_path = '/content/FP_model_assessment_results.xlsx'
results_df.to_excel(results_file_path)

print(f"Model assessment results saved to: {results_file_path}")

#CELL10
import pandas as pd
import numpy as np
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import StratifiedKFold, cross_val_score
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
input_file_path = '/content/fingerprints_output.xlsx'
data_df = pd.read_excel(input_file_path)

# Extract target variable
target_column = 'Classification'  # Adjust this based on your actual target column name
y = data_df[target_column].apply(lambda x: 1 if x == 'positive' else 0)  # Convert to binary labels

# Extract and preprocess fingerprint columns
fingerprint_columns = ['CDK_FP', 'CDK_ExtFP', 'Estate_FP', 'MACCS_FP', 'SubFP', 'PubChem_FP']
fingerprints = {}

for col in fingerprint_columns:
    try:
        fingerprints[col] = np.array([list(map(int, fp.strip('[]').split(','))) for fp in data_df[col]])
    except ValueError as e:
        print(f"Error converting values for column {col}: {e}")

# Define models
models = {
    'SVM': SVC(probability=True),
    'Naive Bayes': GaussianNB(),
    'kNN': KNeighborsClassifier(),
    'Decision Tree': DecisionTreeClassifier(),
    'Random Forest': RandomForestClassifier(),
    'ANN': MLPClassifier(max_iter=1000)
}

# Evaluate models using cross-validation
results = []
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

for descriptor_name, X in fingerprints.items():
    for model_name, model in models.items():
        print(f"Evaluating {model_name} with {descriptor_name} Fingerprints...")
        try:
            if X.size == 0:
                print(f"No data for {descriptor_name} Fingerprints.")
                continue

            cv_scores = cross_val_score(model, X, y, cv=cv, scoring='roc_auc')
            mean_auc = np.mean(cv_scores)
            results.append((model_name, descriptor_name, mean_auc))
        except Exception as e:
            print(f"Failed to evaluate {model_name} with {descriptor_name} Fingerprints: {e}")

# Check if results are empty
if not results:
    print("No results to display.")
else:
    # Sort results by mean AUC in descending order and get top 10 combinations
    results.sort(key=lambda x: x[2], reverse=True)
    top_10_combinations = results[:10]

    # Print top 10 combinations
    print("\nTop 10 Model and Fingerprint Combinations:")
    for model_name, descriptor_name, auc in top_10_combinations:
        print(f"Model: {model_name}, Fingerprint: {descriptor_name}, AUC: {auc:.4f}")

    # Save the results to an Excel file
    results_df = pd.DataFrame(results, columns=['Model', 'Fingerprint', 'AUC'])
    results_file_path = '/content/top_model_fingerprint_combinations.xlsx'
    results_df.to_excel(results_file_path, index=False)
    print(f"Top combinations saved to: {results_file_path}")

    # Define a color palette
    palette = sns.color_palette("muted")

    # Generate a bar graph for all combinations
    plt.figure(figsize=(7, 5))
    bar_width = 0.15
    positions = np.arange(len(fingerprint_columns))

    for i, model_name in enumerate(models.keys()):
        model_results = [r[2] for r in results if r[0] == model_name]
        plt.bar(positions + i * bar_width, model_results, bar_width, label=model_name, color=palette[i % len(palette)])

    plt.title('AUC Scores of Models with Different Fingerprints')
    plt.xlabel('Fingerprints')
    plt.ylabel('AUC Score')
    plt.xticks(positions + bar_width * (len(models) / 2), fingerprint_columns, rotation=45)
    plt.legend()
    plt.tight_layout()
    plt.savefig('/content/model_fingerprint_performance.png')
    plt.show()

    # Generate a bar graph for top 10 combinations
    top_models = [f"{model_name} ({descriptor_name})" for model_name, descriptor_name, _ in top_10_combinations]
    top_scores = [auc for _, _, auc in top_10_combinations]

    plt.figure(figsize=(7, 5))
    plt.barh(top_models, top_scores, color=palette[:10])
    plt.xlabel('AUC Score')
    plt.title('Top 10 Model and Fingerprint Combinations')
    plt.tight_layout()
    plt.savefig('/content/top_10_model_fingerprint_combinations.png')
    plt.show()

"""**DESCRIPTOR PROCESSING**"""

#CELL11
#merging descriptors with dataset
import pandas as pd

# Load the first Excel file
file1 = '/content/DATASET_insilco_prediction.xlsx'  # Replace with the path to your first Excel file
df1 = pd.read_excel(file1)

# Load the second Excel file
file2 = '/content/molecular_descriptors_output.xlsx'  # Replace with the path to your second Excel file
df2 = pd.read_excel(file2)

# Assuming 'SMILES' is the common column
common_column = 'SMILES'  # Replace with the actual common column name

# Merge the dataframes
merged_df = pd.merge(df1, df2, on=common_column, how='inner')

# Save the merged dataframe to a new Excel file
merged_output_file = 'merged_output_descriptors.xlsx'
merged_df.to_excel(merged_output_file, index=False)

# Display the resulting dataframe
print(merged_df.head())

#CELL12
#Correlation Analysis:
import pandas as pd
import numpy as np

# Load the dataset
file_path = '/content/merged_output_descriptors.xlsx'
df = pd.read_excel(file_path)

# Drop the 'No.' column
df = df.drop(columns=['No.'])

# Select only numeric columns
numeric_df = df.select_dtypes(include=[float, int])

# Remove columns with all zero values
numeric_df = numeric_df.loc[:, (numeric_df != 0).any(axis=0)]

# Remove columns with variance lower than 0.05
numeric_df = numeric_df.loc[:, numeric_df.var() >= 0.05]

# Calculate pairwise correlations
correlation_matrix = numeric_df.corr().abs()

# Select upper triangle of correlation matrix
upper_triangle = correlation_matrix.where(
    np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)
)

# Find index of feature columns with correlation greater than 0.95
to_drop = [
    column for column in upper_triangle.columns if any(upper_triangle[column] > 0.95)
]

# Drop highly correlated columns
numeric_df = numeric_df.drop(columns=to_drop)

# Concatenate non-numeric columns back with cleaned numeric data (if any non-numeric columns are needed)
non_numeric_df = df.select_dtypes(exclude=[float, int])
cleaned_df = pd.concat([non_numeric_df, numeric_df], axis=1)

# Save the cleaned dataset
cleaned_file_path = 'cleaned_descriptor_output.xlsx'
cleaned_df.to_excel(cleaned_file_path, index=False)

print("Redundant descriptors with correlation values higher than 0.95 have been removed.")

#CELL13
#Tree-Based Estimators
import pandas as pd
from sklearn.ensemble import RandomForestClassifier

# Load the cleaned dataset
cleaned_file_path = '/content/cleaned_descriptor_output.xlsx'
cleaned_df = pd.read_excel(cleaned_file_path)

# Assuming 'target_column' contains the column name of your target variable
target_column = 'Classification'

# Separate features and target variable
X = cleaned_df.drop(columns=[target_column])
y = cleaned_df[target_column]

# Encode categorical columns using one-hot encoding
X_encoded = pd.get_dummies(X)

# Initialize Random Forest Classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

# Fit the model
rf_classifier.fit(X_encoded, y)

# Get feature importances
feature_importances = rf_classifier.feature_importances_

# Create a DataFrame to store feature importances
importance_df = pd.DataFrame({'Feature': X_encoded.columns, 'Importance': feature_importances})

# Sort the DataFrame by importance
importance_df = importance_df.sort_values(by='Importance', ascending=False)

# Filter descriptors with importance value of 0 and count the number of descriptors removed
filtered_importance_df = importance_df[importance_df['Importance'] != 0]
num_descriptors_removed = len(importance_df) - len(filtered_importance_df)

# Find minimum and maximum importance values
min_importance = importance_df['Importance'].min()
max_importance = importance_df['Importance'].max()

print(f"Minimum Importance: {min_importance}")
print(f"Maximum Importance: {max_importance}")

# Total number of descriptors
total_descriptors = len(X_encoded.columns)
print(f"Total number of features: {total_descriptors}")

# Print the number of descriptors removed
print(f"Number of features removed: {num_descriptors_removed}")

# Print or visualize the filtered feature importances
print(filtered_importance_df)

# Save filtered feature importances to a file
filtered_importance_file_path = '/content/filtered_DES_feature_importances.csv'
filtered_importance_df.to_csv(filtered_importance_file_path, index=False)

# Get the column names to drop
columns_to_drop = filtered_importance_df['Feature'].tolist()

# Drop columns from the dataset
X_filtered = X_encoded.drop(columns=columns_to_drop)

# Save the modified dataset to a new Excel file
filtered_file_path = '/content/filtered_DES_dataset.xlsx'
X_filtered.to_excel(filtered_file_path, index=False)

print(f"Filtered dataset saved to: {filtered_file_path}")
print(f"Filtered feature importance CSV file saved to: {filtered_importance_file_path}")



#CELL14
#Recursive Feature Elimination (RFE)
import pandas as pd
from sklearn.feature_selection import RFE
from sklearn.svm import SVC
from sklearn.model_selection import cross_val_score, StratifiedKFold

# Load the cleaned dataset
cleaned_file_path = '/content/cleaned_descriptor_output.xlsx'
cleaned_df = pd.read_excel(cleaned_file_path)

# Assuming 'Classification' is your target variable
target_column = 'Classification'

# Separate features and target variable
X = cleaned_df.drop(columns=[target_column])
y = cleaned_df[target_column]

# Encode categorical columns using one-hot encoding
X_encoded = pd.get_dummies(X)

# Initialize SVM model with linear kernel
svm_model = SVC(kernel='linear', random_state=42)

# Initialize RFE with the SVM model
rfe = RFE(estimator=svm_model)

# Set up cross-validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Perform RFE with cross-validation to select the optimal number of features
n_features_to_select = None  # None means keep all features
rfe.fit(X_encoded, y)

# Use cross-validation to evaluate the model
scores = cross_val_score(rfe, X_encoded, y, cv=cv)

# Print the cross-validation scores
print(f"Cross-validation scores: {scores}")
print(f"Mean cross-validation score: {scores.mean()}")

# Get the support (selected features) and ranking of features
selected_features = X_encoded.columns[rfe.support_]
ranking = rfe.ranking_

# Create a DataFrame to store the feature rankings
rfe_ranking_df = pd.DataFrame({'Feature': X_encoded.columns, 'Ranking': ranking})

# Sort the DataFrame by ranking (1 means selected feature)
rfe_ranking_df = rfe_ranking_df.sort_values(by='Ranking')

# Print or visualize the RFE rankings
print(rfe_ranking_df)

# Save the RFE rankings to a file
rfe_ranking_file_path = '/content/rfe_DES_feature_rankings.csv'
rfe_ranking_df.to_csv(rfe_ranking_file_path, index=False)

# Drop unselected features from the dataset based on RFE
X_rfe_selected = X_encoded[selected_features]

# Save the modified dataset with selected features to a new Excel file
selected_features_file_path = '/content/rfe_DES_selected_features_dataset.xlsx'
X_rfe_selected.to_excel(selected_features_file_path, index=False)

print(f"RFE selected features dataset saved to: {selected_features_file_path}")
print(f"RFE feature ranking CSV file saved to: {rfe_ranking_file_path}")

#CELL15
#Import Required Libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold
from sklearn.metrics import roc_auc_score
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier

#Load the Dataset with Selected Features:
# Load the dataset with selected features
selected_features_file_path = '/content/rfe_DES_selected_features_dataset.xlsx'
selected_df = pd.read_excel(selected_features_file_path)

# Load the original target column from the cleaned dataset
cleaned_file_path = '/content/cleaned_descriptor_output.xlsx'
cleaned_df = pd.read_excel(cleaned_file_path)
target_column = 'Classification'

# Ensure target variable is the same length as selected features data
y = cleaned_df[target_column][:len(selected_df)]
X = selected_df

#Define Models and Parameter Grids:
models = {
    'SVM': SVC(probability=True),
    'Naive Bayes': GaussianNB(),
    'kNN': KNeighborsClassifier(),
    'Decision Tree': DecisionTreeClassifier(),
    'Random Forest': RandomForestClassifier(),
    'ANN': MLPClassifier(max_iter=1000)
}

param_grids = {
    'SVM': {'kernel': ['rbf'], 'C': [0.1, 1, 10], 'gamma': [0.01, 0.1, 1]},
    'Naive Bayes': {},  # No hyperparameters to optimize
    'kNN': {'n_neighbors': [3, 5, 7, 9]},
    'Decision Tree': {'criterion': ['gini', 'entropy'], 'max_depth': [None, 10, 20, 30]},
    'Random Forest': {'n_estimators': [50, 100, 200], 'criterion': ['gini', 'entropy']},
    'ANN': {'hidden_layer_sizes': [(50,), (100,), (50, 50)], 'activation': ['tanh', 'relu'], 'solver': ['adam', 'sgd']}
}

#CELL16
#Model Building and Evaluation:
results = {}
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

for model_name in models:
    print(f"Training {model_name}...")
    model = models[model_name]
    param_grid = param_grids[model_name]

    if param_grid:
        grid_search = GridSearchCV(model, param_grid, cv=cv, scoring='roc_auc')
        grid_search.fit(X, y)
        best_model = grid_search.best_estimator_
        best_params = grid_search.best_params_
    else:
        best_model = model
        best_model.fit(X, y)
        best_params = {}

    # Cross-validated AUC score
    auc_scores = cross_val_score(best_model, X, y, cv=cv, scoring='roc_auc')
    results[model_name] = {'best_params': best_params, 'mean_auc': np.mean(auc_scores)}

# Display results
for model_name, result in results.items():
    print(f"{model_name}: Best Params - {result['best_params']}, Mean AUC - {result['mean_auc']:.4f}")

#Summary and Visualization
import matplotlib.pyplot as plt

model_names = list(results.keys())
mean_aucs = [results[name]['mean_auc'] for name in model_names]

plt.barh(model_names, mean_aucs, color='skyblue')
plt.xlabel('Mean AUC')
plt.title('Model Comparison based on AUC')
plt.show()

#Save Results:
results_df = pd.DataFrame(results).T
results_file_path = '/content/final_DES_model_results.xlsx'
results_df.to_excel(results_file_path)

print(f"Model results saved to: {results_file_path}")

#CELL17
#Define Metric Calculation Functions
from sklearn.metrics import confusion_matrix, roc_auc_score

def calculate_metrics(y_true, y_pred):
    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()
    accuracy = (tp + tn) / (tp + tn + fp + fn)
    specificity = tn / (tn + fp)
    sensitivity = tp / (tp + fn)

    return {
        'TP': tp,
        'TN': tn,
        'FP': fp,
        'FN': fn,
        'Accuracy': accuracy,
        'Specificity': specificity,
        'Sensitivity': sensitivity
    }

def evaluate_model(model, X, y, cv):
    auc_scores = cross_val_score(model, X, y, cv=cv, scoring='roc_auc')
    preds = cross_val_score(model, X, y, cv=cv, scoring='accuracy')

    return {
        'AUC': np.mean(auc_scores),
        'Accuracy': np.mean(preds)
    }

#Calculate Metrics for Each Model Using Cross-Validation
results = {}
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

for model_name in models:
    print(f"Evaluating {model_name}...")
    model = models[model_name]
    param_grid = param_grids[model_name]

    if param_grid:
        grid_search = GridSearchCV(model, param_grid, cv=cv, scoring='roc_auc')
        grid_search.fit(X, y)
        best_model = grid_search.best_estimator_
        best_params = grid_search.best_params_
    else:
        best_model = model
        best_model.fit(X, y)
        best_params = {}

    # Perform cross-validation to calculate AUC and other metrics
    evaluation_scores = evaluate_model(best_model, X, y, cv)

    # Fit model on full data for confusion matrix metrics
    best_model.fit(X, y)
    y_pred = best_model.predict(X)
    metrics = calculate_metrics(y, y_pred)
    metrics['AUC'] = evaluation_scores['AUC']

    results[model_name] = {
        'best_params': best_params,
        'metrics': metrics
    }

# Display results
for model_name, result in results.items():
    metrics = result['metrics']
    print(f"\n{model_name} - Best Params: {result['best_params']}")
    print(f"AUC: {metrics['AUC']:.4f}")
    print(f"Accuracy: {metrics['Accuracy']:.4f}")
    print(f"Specificity: {metrics['Specificity']:.4f}")
    print(f"Sensitivity: {metrics['Sensitivity']:.4f}")
    print(f"TP: {metrics['TP']}, TN: {metrics['TN']}, FP: {metrics['FP']}, FN: {metrics['FN']}")

# Summary and Save Results
results_df = pd.DataFrame.from_dict({model: data['metrics'] for model, data in results.items()}, orient='index')
results_file_path = '/content/DES_model_assessment_results.xlsx'
results_df.to_excel(results_file_path)

print(f"Model assessment results saved to: {results_file_path}")

#CELL18
import pandas as pd
from rdkit import Chem
from rdkit.Chem import Descriptors

# Load the input Excel file
input_file_path = '/content/DATASET_insilco_prediction.xlsx'
data_df = pd.read_excel(input_file_path)

# Define functions to generate descriptors
def generate_constitutional_descriptors(molecule):
    mol = Chem.MolFromSmiles(molecule)
    return {
        'NumAtoms': mol.GetNumAtoms(),
        'NumBonds': mol.GetNumBonds(),
        'NumHeteroatoms': Descriptors.NumHeteroatoms(mol),
        'NumRotatableBonds': Descriptors.NumRotatableBonds(mol),
        'NumAromaticAtoms': len([atom for atom in mol.GetAtoms() if atom.GetIsAromatic()])
    }

def generate_basak_descriptors(molecule):
    mol = Chem.MolFromSmiles(molecule)
    return {
        'BertzCT': Descriptors.BertzCT(mol),
        'BalabanJ': Descriptors.BalabanJ(mol),
        'HallKierAlpha': Descriptors.HallKierAlpha(mol),
        'Ipc': Descriptors.Ipc(mol)
    }

def generate_burden_descriptors(molecule):
    mol = Chem.MolFromSmiles(molecule)
    return {
        'BalabanJ': Descriptors.BalabanJ(mol),
        'BertzCT': Descriptors.BertzCT(mol),
        'HallKierAlpha': Descriptors.HallKierAlpha(mol),
        'Kappa1': Descriptors.Kappa1(mol),
        'Kappa2': Descriptors.Kappa2(mol)
    }

# Apply the descriptor generation functions to each molecule in the dataset
data_df['ConstitutionalDescriptors'] = data_df['SMILES'].apply(generate_constitutional_descriptors)
data_df['BasakDescriptors'] = data_df['SMILES'].apply(generate_basak_descriptors)
data_df['BurdenDescriptors'] = data_df['SMILES'].apply(generate_burden_descriptors)

# Save the modified dataset with descriptors to a new Excel file
output_file_path = '/content/DATASET_with_descriptors.xlsx'
data_df.to_excel(output_file_path, index=False)

print(f"Descriptors added and dataset saved to: {output_file_path}")

#CELL19
import pandas as pd
import numpy as np
from rdkit import Chem
from rdkit.Chem import Descriptors, MACCSkeys
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import StratifiedKFold, cross_val_score
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
input_file_path = '/content/DATASET_insilco_prediction.xlsx'
data_df = pd.read_excel(input_file_path)

# Define descriptor generation functions
def generate_constitutional_descriptors(smiles):
    mol = Chem.MolFromSmiles(smiles)
    return {
        'NumAtoms': Descriptors.HeavyAtomCount(mol),
        'NumHeteroatoms': Descriptors.NumHeteroatoms(mol),
        'NumRotatableBonds': Descriptors.NumRotatableBonds(mol),
        'NumAromaticRings': Descriptors.NumAromaticRings(mol),
        'MolWt': Descriptors.MolWt(mol)
    }

def generate_basak_descriptors(smiles):
    mol = Chem.MolFromSmiles(smiles)
    return {
        'BertzCT': Descriptors.BertzCT(mol),
        'BalabanJ': Descriptors.BalabanJ(mol),
        'HallKierAlpha': Descriptors.HallKierAlpha(mol)
    }

def generate_burden_descriptors(smiles):
    mol = Chem.MolFromSmiles(smiles)
    # Generating MACCS keys as a placeholder for burden descriptors
    maccs_keys = MACCSkeys.GenMACCSKeys(mol)
    return {f'MACCS_{i}': int(maccs_keys.GetBit(i)) for i in range(len(maccs_keys))}

# Placeholder function for CATS descriptors
def generate_cats_descriptors(smiles):
    mol = Chem.MolFromSmiles(smiles)
    return {
        'CATS1': Descriptors.TPSA(mol),  # Just a placeholder, replace with actual CATS descriptors
        'CATS2': Descriptors.MolLogP(mol),  # Just a placeholder, replace with actual CATS descriptors
        'CATS3': Descriptors.MolMR(mol)  # Just a placeholder, replace with actual CATS descriptors
    }

def generate_moe_descriptors(smiles):
    mol = Chem.MolFromSmiles(smiles)
    return {
        'AMW': Descriptors.MolWt(mol),  # Approximation of MOE's AMW
        'LogP': Descriptors.MolLogP(mol),  # MOE's LogP
        'TPSA': Descriptors.TPSA(mol)  # MOE's TPSA
    }

# Apply the descriptor generation functions to each molecule in the dataset
data_df['ConstitutionalDescriptors'] = data_df['SMILES'].apply(generate_constitutional_descriptors)
data_df['BasakDescriptors'] = data_df['SMILES'].apply(generate_basak_descriptors)
data_df['BurdenDescriptors'] = data_df['SMILES'].apply(generate_burden_descriptors)
data_df['CATSDescriptors'] = data_df['SMILES'].apply(generate_cats_descriptors)
data_df['MOEDescriptors'] = data_df['SMILES'].apply(generate_moe_descriptors)

# Extract target variable
target_column = 'Classification'  # Adjust this based on your actual target column name
y = data_df[target_column].apply(lambda x: 1 if x == 'positive' else 0)  # Convert to binary labels

# Extract and normalize descriptors
constitutional_descriptors = pd.json_normalize(data_df['ConstitutionalDescriptors'])
basak_descriptors = pd.json_normalize(data_df['BasakDescriptors'])
burden_descriptors = pd.json_normalize(data_df['BurdenDescriptors'])
cats_descriptors = pd.json_normalize(data_df['CATSDescriptors'])
moe_descriptors = pd.json_normalize(data_df['MOEDescriptors'])

# Convert descriptor DataFrames to NumPy arrays
constitutional_descriptors = constitutional_descriptors.values
basak_descriptors = basak_descriptors.values
burden_descriptors = burden_descriptors.values
cats_descriptors = cats_descriptors.values
moe_descriptors = moe_descriptors.values

# Define models
models = {
    'SVM': SVC(probability=True),
    'Naive Bayes': GaussianNB(),
    'kNN': KNeighborsClassifier(),
    'Decision Tree': DecisionTreeClassifier(),
    'Random Forest': RandomForestClassifier(),
    'ANN': MLPClassifier(max_iter=1000)
}

descriptor_sets = {
    'Constitutional': constitutional_descriptors,
    'Basak': basak_descriptors,
    'Burden': burden_descriptors,
    'CATS': cats_descriptors,
    'MOE': moe_descriptors
}

# Evaluate models using cross-validation
results = []
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

for descriptor_name, X in descriptor_sets.items():
    for model_name, model in models.items():
        print(f"Evaluating {model_name} with {descriptor_name} Descriptors...")
        try:
            if X.size == 0:
                print(f"No data for {descriptor_name} Descriptors.")
                continue

            cv_scores = cross_val_score(model, X, y, cv=cv, scoring='roc_auc')
            mean_auc = np.mean(cv_scores)
            results.append((model_name, descriptor_name, mean_auc))
        except Exception as e:
            print(f"Failed to evaluate {model_name} with {descriptor_name} Descriptors: {e}")

# Check if results are empty
if not results:
    print("No results to display.")
else:
    # Sort results by mean AUC in descending order and get top 5 combinations
    results.sort(key=lambda x: x[2], reverse=True)
    top_5_combinations = results[:5]

    # Print top 5 combinations
    print("\nTop 5 Model and Descriptor Combinations:")
    for model_name, descriptor_name, auc in top_5_combinations:
        print(f"Model: {model_name}, Descriptor: {descriptor_name}, AUC: {auc:.4f}")

    # Save the results to an Excel file
    results_df = pd.DataFrame(results, columns=['Model', 'Descriptor', 'AUC'])
    results_file_path = '/content/top_model_descriptor_combinations.xlsx'
    results_df.to_excel(results_file_path, index=False)
    print(f"Top combinations saved to: {results_file_path}")

    # Define a color palette
    palette = sns.color_palette("muted")

    # Generate a bar graph for all combinations
    plt.figure(figsize=(7, 5))
    bar_width = 0.15
    positions = np.arange(len(descriptor_sets))

    for i, model_name in enumerate(models.keys()):
        model_results = [r[2] for r in results if r[0] == model_name]
        plt.bar(positions + i * bar_width, model_results, bar_width, label=model_name, color=palette[i % len(palette)])

    plt.title('AUC Scores of Models with Different Descriptors')
    plt.xlabel('Descriptors')
    plt.ylabel('AUC Score')
    plt.xticks(positions + bar_width * (len(models) / 2), descriptor_sets.keys(), rotation=45)
    plt.legend()
    plt.tight_layout()
    plt.savefig('/content/model_descriptor_performance.png')
    plt.show()

    # Generate a bar graph for top 5 combinations
    top_models = [f"{model_name} ({descriptor_name})" for model_name, descriptor_name, _ in top_5_combinations]
    top_scores = [auc for _, _, auc in top_5_combinations]

    plt.figure(figsize=(7, 5))
    plt.barh(top_models, top_scores, color=palette[:5])
    plt.xlabel('AUC Score')
    plt.title('Top 5 Model and Descriptor Combinations')
    plt.tight_layout()
    plt.savefig('/content/top_5_model_descriptor_combinations.png')
    plt.show()

##CELL20
##AD NEW
import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_val_score
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from rdkit import Chem
from rdkit.Chem import MACCSkeys, DataStructs
import matplotlib.pyplot as plt

# Load the dataset with selected features
selected_features_file_path = '/content/rfe_DES_selected_features_dataset.xlsx'
selected_df = pd.read_excel(selected_features_file_path)

# Load the original target column from the cleaned dataset
cleaned_file_path = '/content/cleaned_descriptor_output.xlsx'
cleaned_df = pd.read_excel(cleaned_file_path)
target_column = 'Classification'

# Ensure target variable is the same length as selected features data
y = cleaned_df[target_column][:len(selected_df)]
X = selected_df

# Define models and parameter grids
models = {
    'SVM': SVC(probability=True),
    'Naive Bayes': GaussianNB(),
    'kNN': KNeighborsClassifier(),
    'Decision Tree': DecisionTreeClassifier(),
    'Random Forest': RandomForestClassifier(),
    'ANN': MLPClassifier(max_iter=1000)
}

param_grids = {
    'SVM': {'kernel': ['rbf'], 'C': [0.1, 1, 10], 'gamma': [0.01, 0.1, 1]},
    'Naive Bayes': {},  # No hyperparameters to optimize
    'kNN': {'n_neighbors': [3, 5, 7, 9]},
    'Decision Tree': {'criterion': ['gini', 'entropy'], 'max_depth': [None, 10, 20, 30]},
    'Random Forest': {'n_estimators': [50, 100, 200], 'criterion': ['gini', 'entropy']},
    'ANN': {'hidden_layer_sizes': [(50,), (100,), (50, 50)], 'activation': ['tanh', 'relu'], 'solver': ['adam', 'sgd']}
}

# Load molecule SMILES strings (assuming they are in the cleaned_df)
smiles_column = 'SMILES'  # Change this to the actual column name if different
smiles_list = cleaned_df[smiles_column].tolist()

# Calculate MACCS fingerprints
def get_fingerprint(smiles):
    mol = Chem.MolFromSmiles(smiles)
    if mol is not None:
        return MACCSkeys.GenMACCSKeys(mol)
    else:
        return None

fingerprints = [get_fingerprint(smiles) for smiles in smiles_list]

# Calculate the Tanimoto coefficients for a given query molecule
def calculate_tanimoto_coefficients(query_fp, training_fps):
    return [DataStructs.FingerprintSimilarity(query_fp, fp) for fp in training_fps if fp is not None]

# Average Tanimoto coefficient for each molecule in the dataset
average_tanimotos = []
all_tanimoto_coeffs = []

for i, fp in enumerate(fingerprints):
    if fp is not None:
        tanimoto_coeffs = calculate_tanimoto_coefficients(fp, fingerprints[:i] + fingerprints[i+1:])
        avg_t = np.mean(tanimoto_coeffs)
        average_tanimotos.append(avg_t)
        all_tanimoto_coeffs.extend(tanimoto_coeffs)
        print(f"Molecule {i}: Average Tanimoto Coefficient - {avg_t}")

# Calculate and print the range of Tanimoto coefficients
tanimoto_range = (np.min(all_tanimoto_coeffs), np.max(all_tanimoto_coeffs))
print(f"Range of Tanimoto Coefficients: {tanimoto_range}")

# Calculate and print the range of average Tanimoto coefficients
average_tanimoto_range = (np.min(average_tanimotos), np.max(average_tanimotos))
print(f"Range of Average Tanimoto Coefficients: {average_tanimoto_range}")

print("Distribution of Average Tanimoto Coefficients:")
print(np.histogram(average_tanimotos))

# Define thresholds to categorize chemicals into ID and OD
thresholds = [0, 0.17, 0.16, 0.15, 0.14]

# Function to calculate ID and OD indices for a given threshold
def get_in_out_domain_indices(threshold):
    in_domain_indices = [i for i, avg_t in enumerate(average_tanimotos) if avg_t >= threshold]
    out_of_domain_indices = [i for i, avg_t in enumerate(average_tanimotos) if avg_t < threshold]
    return in_domain_indices, out_of_domain_indices

# Model performance evaluation
results = {model_name: {t: {'ID': [], 'OD': []} for t in thresholds} for model_name in models}
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

for threshold in thresholds:
    in_domain_indices, out_of_domain_indices = get_in_out_domain_indices(threshold)

    for model_name in models:
        print(f"Training {model_name} at threshold {threshold}...")
        model = models[model_name]
        param_grid = param_grids[model_name]

        if param_grid:
            grid_search = GridSearchCV(model, param_grid, cv=cv, scoring='roc_auc')
            grid_search.fit(X, y)
            best_model = grid_search.best_estimator_
            best_params = grid_search.best_params_
        else:
            best_model = model
            best_model.fit(X, y)
            best_params = {}

        # Cross-validated AUC score for ID and OD chemicals
        if in_domain_indices:
            auc_scores_id = cross_val_score(best_model, X.iloc[in_domain_indices], y.iloc[in_domain_indices], cv=cv, scoring='roc_auc')
            results[model_name][threshold]['ID'] = auc_scores_id
        if out_of_domain_indices:
            auc_scores_od = cross_val_score(best_model, X.iloc[out_of_domain_indices], y.iloc[out_of_domain_indices], cv=cv, scoring='roc_auc')
            results[model_name][threshold]['OD'] = auc_scores_od

# Plot the results
plt.figure(figsize=(15, 10))

# Plot AUC values for ID chemicals (A)
plt.subplot(2, 2, 1)
for model_name in models:
    aucs = [np.mean(results[model_name][t]['ID']) for t in thresholds]
    plt.plot(thresholds, aucs, label=model_name, marker='o')
plt.xlabel('Threshold')
plt.ylabel('Mean AUC')
plt.title('AUC values of ID chemicals (MACCS fingerprint)')
plt.legend()

# Plot AUC values for OD chemicals (B)
plt.subplot(2, 2, 2)
for model_name in models:
    aucs = [np.mean(results[model_name][t]['OD']) for t in thresholds]
    plt.plot(thresholds, aucs, label=model_name, marker='o')
plt.xlabel('Threshold')
plt.ylabel('Mean AUC')
plt.title('AUC values of OD chemicals (MACCS fingerprint)')
plt.legend()


# Save Results
results_df = pd.DataFrame(results).T
results_file_path = '/content/NEW_final_DES_model_with_AD.xlsx'
results_df.to_excel(results_file_path)

print(f"Model results saved to: {results_file_path}")

#CELL21
#heatmap & mol wt

import pandas as pd
from rdkit import Chem
from rdkit.Chem import Descriptors, MACCSkeys, DataStructs
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Load your datasets (training set and external validation set)
training_set_path = '/content/traindata_insilco_prediction.xlsx'
validation_set_path = '/content/testdata_insilico_prediction.xlsx'

training_df = pd.read_excel(training_set_path)
validation_df = pd.read_excel(validation_set_path)

# Function to calculate MW and ALogP from SMILES
def calculate_properties(smiles):
    mol = Chem.MolFromSmiles(smiles)
    if mol:
        mw = Descriptors.MolWt(mol)
        alogp = Descriptors.MolLogP(mol)
        return mw, alogp
    else:
        return None, None

# Apply the function to each row in the datasets
training_df['MW'], training_df['ALogP'] = zip(*training_df['SMILES'].apply(calculate_properties))
validation_df['MW'], validation_df['ALogP'] = zip(*validation_df['SMILES'].apply(calculate_properties))

# Save the updated datasets with MW and ALogP
training_df.to_excel('/content/training_set_with_properties.xlsx', index=False)
validation_df.to_excel('/content/validation_set_with_properties.xlsx', index=False)

print(f"Training set with MW and ALogP saved to: /content/training_set_with_properties.xlsx")
print(f"Validation set with MW and ALogP saved to: /content/validation_set_with_properties.xlsx")

# Plot the diversity distribution
plt.figure(figsize=(5, 4))  # Adjusted figure size
plt.scatter(training_df['MW'], training_df['ALogP'], c='black', label='Training Set', marker='s')
plt.scatter(validation_df['MW'], validation_df['ALogP'], c='red', label='Validation Set', marker='o')
plt.xlabel('Molecular Weight (MW)')
plt.ylabel('ALogP')
plt.title('Diversity Distribution of Training and Validation Sets')
plt.legend()
plt.show()

# Combine the datasets for similarity calculation
combined_df = pd.concat([training_df, validation_df], ignore_index=True)
smiles_list = combined_df['SMILES'].tolist()

# Function to get MACCS fingerprint from SMILES
def get_fingerprint(smiles):
    mol = Chem.MolFromSmiles(smiles)
    if mol is not None:
        return MACCSkeys.GenMACCSKeys(mol)
    else:
        return None

# Calculate MACCS fingerprints
fingerprints = [get_fingerprint(smiles) for smiles in smiles_list]

# Calculate the Tanimoto similarity matrix
n_molecules = len(fingerprints)
similarity_matrix = np.zeros((n_molecules, n_molecules))

for i in range(n_molecules):
    for j in range(n_molecules):
        if fingerprints[i] is not None and fingerprints[j] is not None:
            similarity_matrix[i, j] = DataStructs.FingerprintSimilarity(fingerprints[i], fingerprints[j])

# Plot the heatmap
plt.figure(figsize=(5,4))  # Adjusted figure size
sns.heatmap(similarity_matrix, cmap='viridis')
plt.title('Heatmap of Molecular Similarity (Tanimoto Coefficient)')
plt.xlabel('Molecules')
plt.ylabel('Molecules')
plt.show()



